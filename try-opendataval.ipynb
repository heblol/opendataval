{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook to try out opendataval\n",
    "\n",
    "Here are the 4 interacting parts of opendataval\n",
    "1. `DataFetecher`, Loads data and holds meta data regarding splits\n",
    "2. `Model`, trainable prediction model.\n",
    "3. `DataEvaluator`, measures the data values of input data point for a specified model.\n",
    "4. `ExperimentMediator`, facilitates experiments regarding data values across several `DataEvaluators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Fetcher\n",
    "from opendataval.dataloader import DataFetcher\n",
    "\n",
    "available_ds = DataFetcher.datasets_available()  # ['dataset_name1', 'dataset_name2']\n",
    "\n",
    "print(available_ds)\n",
    "\n",
    "fetcher = DataFetcher(dataset_name='iris')\n",
    "fetcher = fetcher.split_dataset_by_count(70, 20, 10)\n",
    "\n",
    "# Compared to the introduction, I do not want to add noise in the example.\n",
    "# fetcher = fetcher.noisify(mix_labels, noise_rate=.1)\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = fetcher.datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataval.model import LogisticRegression\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "model.fit(\n",
    "    x_train=x_train, \n",
    "    y_train=y_train)\n",
    "\n",
    "predictions = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataval.dataval.ame import AME\n",
    "\n",
    "dataval = (\n",
    "    AME(num_models=8000)\n",
    "    .train(fetcher=fetcher, pred_model=model, metric=metric)\n",
    ")\n",
    "\n",
    "data_values = dataval.data_values  # Cached values\n",
    "data_values = dataval.evaluate_data_values()  # Recomputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line model metric_name=None: perf=0.653333306312561\n",
      "Start: marginal contribution computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_stat=1.005900504280515\n",
      "Done: marginal contribution computation\n",
      "Elapsed time DataShapley(): 0:04:57.407580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "from opendataval.experiment import ExperimentMediator\n",
    "from opendataval.dataval.margcontrib import DataShapley\n",
    "\n",
    "exper_med: ExperimentMediator = ExperimentMediator.model_factory_setup(\n",
    "    dataset_name='iris',\n",
    "    force_download=False,\n",
    "    train_count=50,\n",
    "    valid_count=25,\n",
    "    test_count=75,\n",
    "    model_name='ClassifierMLP',\n",
    "    train_kwargs={'epochs': 5, 'batch_size': 20},\n",
    ")\n",
    "\n",
    "\n",
    "dataval = DataShapley()\n",
    "# .train(\n",
    "#     fetcher=fetcher,\n",
    "#     pred_model=model,\n",
    "#     metric=metric,\n",
    "# )\n",
    "\n",
    "list_of_data_evaluators = [DataShapley()]  # Define evaluators here\n",
    "eval_med = exper_med.compute_data_values(list_of_data_evaluators)\n",
    "\n",
    "# Runs a discover the noisy data experiment for each DataEvaluator and plots\n",
    "# data, fig = eval_med.plot(discover_corrupted_sample)\n",
    "\n",
    "# Runs non-plottable experiment\n",
    "# data = eval_method.evaluate(noisy_detection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
